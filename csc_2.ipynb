{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gveloso9983/CSC/blob/main/csc_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKsjihe-XEZK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#for replicability purposes\n",
        "tf.random.set_seed(91195003)\n",
        "np.random.seed(91190530)\n",
        "#for an easy reset backend session state\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "################\n",
        "#1ª PARTE\n",
        "################\n",
        "\n",
        "#Load dataset\n",
        "def load_dataset(path):\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "#split data into training and validation sets\n",
        "def split_data(training, perc=10):\n",
        "    train_idx = np.arange(0, int(len(training)*(100-perc)/100))\n",
        "    val_idx = np.arange(int(len(training)*(100-perc)/100+1), len(training))\n",
        "    return train_idx, val_idx\n",
        "\n",
        "\n",
        "#Plot time series data\n",
        "def plot_confirmed_cases(data):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(range(len(data)), data)\n",
        "    plt.title('Confirmed Cases of COVID-19')\n",
        "    plt.ylabel('Cases')\n",
        "    plt.xlabel('Days')\n",
        "    plt.show()\n",
        "\n",
        "def data_normalization(df, norm_range=(-1, 1)):\n",
        "    #[-1, 1] for LSTM due to the internal use of tanh by the memory cell\n",
        "    scaler = MinMaxScaler(feature_range=norm_range)\n",
        "    #df[['cases']] = scaler.fit_transform(df[['cases']])\n",
        "    df[['Nr_acidentes']] = scaler.fit_transform(df[['Nr_acidentes']])\n",
        "    return scaler\n",
        "\n",
        "#plot learning curve\n",
        "def plot_learning_curves(hist_list, approach):\n",
        "    pass\n",
        "\n",
        "################\n",
        "#2ª PARTE\n",
        "################\n",
        "\n",
        "# build our supervised problem\n",
        "#Preparing the dataset for the LSTM\n",
        "def to_supervised(df, timesteps):\n",
        "    data = df.values\n",
        "    #print(data)\n",
        "    X, y = list(), list()\n",
        "\n",
        "    #iterate over the training set to create X and y, X é um array com 5_timesteps, y será um array com o valor seguinte ao 5 timestep\n",
        "    dataset_size = len(data)\n",
        "\n",
        "    for curr_pos in range(dataset_size):\n",
        "        #end of the input sequence is the current position + the number of timesteps of the input sequence\n",
        "        input_index = curr_pos + timesteps\n",
        "        #end of the labels corresponds to the end of the input sequence + 1\n",
        "        label_index = input_index + 1\n",
        "        #if we have enough data for this sequence\n",
        "        if label_index < dataset_size:\n",
        "            X.append(data[curr_pos:input_index, :])\n",
        "            y.append(data[input_index:label_index, 0])\n",
        "\n",
        "    #print(X)\n",
        "    #print(y)\n",
        "\n",
        "    #using np.float32 for GPU performance\n",
        "    return np.array(X).astype('float32'), np.array(y).astype('float32')\n",
        "\n",
        "\n",
        "################\n",
        "#3ª PARTE\n",
        "################\n",
        "\n",
        "#Building the model\n",
        "def rmse(y_true, y_pred):\n",
        "    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n",
        "\n",
        "\n",
        "def build_model(timesteps, features, h_neurons=64, activation='tanh'):\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.LSTM(h_neurons, input_shape=(timesteps, features)))\n",
        "    model.add(tf.keras.layers.Dense(h_neurons, activation=activation))\n",
        "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "\n",
        "    #model summary (and save it as PNG)\n",
        "    tf.keras.utils.plot_model(model, 'covid19_model.png', show_shapes=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "################\n",
        "#4ª PARTE\n",
        "################\n",
        "#Compiling and fit the model\n",
        "\n",
        "def compile_and_fit(model, epochs, batch_size):\n",
        "\n",
        "    # compile\n",
        "    model.compile(loss=rmse, optimizer=tf.keras.optimizers.Adam(), metrics=['mae', rmse])\n",
        "\n",
        "    # fit\n",
        "    hist_list = list()\n",
        "    loss_list = list()\n",
        "\n",
        "    # Time Series Cross Validator\n",
        "    tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
        "    #print(tscv.split(X))\n",
        "\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        train_idx, val_idx = split_data(train_index, perc=10)  # further split into training and validation sets\n",
        "        # build data\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_val, y_val = X[val_idx], y[val_idx]\n",
        "        X_test, y_test = X[test_index], y[test_index]\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                        epochs=epochs, batch_size=batch_size, shuffle=False)\n",
        "        metrics = model.evaluate(X_test, y_test)\n",
        "        hist_list.append(history)\n",
        "        loss_list.append(metrics[2])\n",
        "\n",
        "    #plot_learning_curves(hist_list, approach='history')\n",
        "    #plot_learning_curves(loss_list, approach='loss')\n",
        "    return model, hist_list, loss_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################\n",
        "#5ª PARTE - Previsão para os próximos 5 dias\n",
        "################\n",
        "#Recursive Multi-Step Forecast!!!\n",
        "def forecast(model, df, timesteps, multisteps, scaler):\n",
        "    input_seq = df[-timesteps:].values #getting the last sequence of known value\n",
        "    inp = input_seq\n",
        "    forecasts = list()\n",
        "\n",
        "    #multisteps tells us how many iterations we want to perform, i.e., how many days we want to predict\n",
        "    for step in range(1, multisteps+1):\n",
        "        inp = inp.reshape(1,timesteps,1)\n",
        "        yhat = model.predict(inp) #dá o valor predito normalizado\n",
        "        yhat_desnormalized = scaler.inverse_transform(yhat) #dá valor predito desnormalizado\n",
        "        forecasts.append(yhat_desnormalized) #adicionar previsao à lista final de previsões\n",
        "        #preparar novo input para fazer previsão para o dia seguinte\n",
        "        inp= np.append(inp[0],yhat) #adiciona previsão recente ao input\n",
        "        inp = inp[-timesteps:] #vai ao input buscar os ultimos timesteps registados\n",
        "\n",
        "    return forecasts\n",
        "\n",
        "def plot_forecast(data, forecasts):\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    #print(\"Zerro len : \",len(data))\n",
        "    plt.plot(range(len(data)), data, color='green', label='Confirmed')\n",
        "\n",
        "    #print(\"Primeiro len : \",(len(data) - 1))\n",
        "    #print(\"Segundoo len : \",len(forecasts) - 1)\n",
        "\n",
        "    #x=range(len(data)-1, len(data)+len(forecasts)-1)\n",
        "\n",
        "    #for xs in x:\n",
        "    #    print(xs)\n",
        "    fi=[]\n",
        "    for f in forecasts:\n",
        "        fi.append(f[0][0])\n",
        "        print(f)\n",
        "    print(fi)\n",
        "\n",
        "    #plt.plot(range(len(data)-1, len(data)+len(forecasts)-1), forecasts, color='red', label='Forecasts')\n",
        "    plt.plot(range(len(data)-1, len(data)+len(forecasts)-1), fi, color='red', label='Forecasts')\n",
        "    plt.title('Confirmed Cases of COVID-19')\n",
        "    plt.ylabel('Cases')\n",
        "    plt.xlabel('Days')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Main Execution\n",
        "timesteps = 7  # number of days that make up a sequence\n",
        "univariate = 1  # number of features used by the model (using conf. cases to predict conf. cases)\n",
        "multisteps = 2  # number of days to forecast – we will forecast the next 5 days\n",
        "cv_splits = 3  # time series cross validator\n",
        "epochs = 25\n",
        "batch_size = 7  # 7 sequences of 5 days - which corresponds to a window of 7 days in a batch\n",
        "path = 'time_series_covid19_confirmed_global.csv'\n",
        "#########################\n",
        "df_raw = pd.read_csv(\"dataset_test.csv\")\n",
        "df_raw[\"Data\"] = pd.to_datetime(df_raw[\"Data\"])\n",
        "df_raw = df_raw.sort_values(\"Data\")\n",
        "df_raw = df_raw.set_index(\"Data\")\n",
        "\n",
        "#########################\n",
        "\n",
        "\n",
        "\n",
        "df = df_raw.copy()\n",
        "#print(df_data.dtypes)\n",
        "#plot_confirmed_cases(df_data)  # the plot you saw previously\n",
        "scaler = data_normalization(df)  # scaling data to [-1, 1]\n",
        "#print(df.head())\n",
        "\n",
        "# our supervised problem\n",
        "X, y = to_supervised(df, timesteps)\n",
        "print(\"Training shape:\", X.shape)\n",
        "print(\"Training labels shape:\", y.shape)\n",
        "\n",
        "# fitting the model\n",
        "model = build_model(timesteps, univariate)\n",
        "model, hist_list, loss_list = compile_and_fit(model, epochs, batch_size)\n",
        "\n",
        "    # Now that we have “tuned” our model, we should retrain it with all the available data\n",
        "    # (as we did in SBS) and obtain real predictions for tomorrow, the day after, …\n",
        "    # We want to forecast the next five days after today!\n",
        "\n",
        "\n",
        "# Recursive Multi-Step Forecast!!!\n",
        "forecasts = forecast(model, df, timesteps, multisteps, scaler)\n",
        "for forecast in forecasts:\n",
        "  print(forecast)\n",
        "\n",
        "#plot_forecast(df_data, forecasts)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}